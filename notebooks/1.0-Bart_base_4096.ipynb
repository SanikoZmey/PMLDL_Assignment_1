{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `In some places code can be simplified/removed/replaced`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all necessary packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from datasets import Dataset\n",
    "from transformers import (DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, \n",
    "                          Seq2SeqTrainer, BartForConditionalGeneration, BartTokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/filtered.tsv\", sep=\"\\t\")\n",
    "toxic = df[df['ref_tox'] > df['trn_tox']].iloc[:, 1:]   # Choosing the partition where the toxicity score of the reference is bigger than the one of its translation\n",
    "toxic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling an example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you don't stop laughing, you'll end up dead... just like your idiot hyena cousins!\n",
      "if you don't stop laughing, you come as your cousins of hyenas.\n"
     ]
    }
   ],
   "source": [
    "print(toxic['reference'].values[100])\n",
    "print(toxic['translation'].values[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking only a part of data with references & their translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sins = toxic[[\"reference\", 'translation']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here it is possible to configure the partition of data that will be taken from all samples for traing(was useful for \"fast check\" of the training process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_factor = sins.shape[0]\n",
    "# part_factor = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the maximum lenght of the toxic sentence for model generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens_r = sins[:part_factor]['reference'].str.lower().str.split().apply(lambda x: len(x))\n",
    "lens_t = sins[:part_factor]['translation'].str.lower().str.split().apply(lambda x: len(x))\n",
    "max_len = np.max([lens_r.max(), lens_t.max()])\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataset to use .map() method for preparing data for the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(sins[:part_factor]).train_test_split(test_size=0.2)   # Create dataset and split it on \"train\" and \"test\" parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and tokenizer, tokenizing data, creating model and data collator objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14653de4579d49828be616f913b63e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/255313 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c741a4da2d457780ac9b9feeed4556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63829 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"ccdv/lsg-bart-base-4096\"  # Model to load - \"bart-base-4096\"\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(checkpoint)   # Creating tokenizer\n",
    "\n",
    "# Function for preprocessing sentences\n",
    "def preprocess_function(data):\n",
    "    inputs = [sent.lower().split() for sent in data[\"reference\"]]\n",
    "    targets = [sent.lower().split() for sent in data[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_len, truncation=True, is_split_into_words=True)\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenizing the data in the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Creating the model's object\n",
    "model = BartForConditionalGeneration.from_pretrained(checkpoint, forced_bos_token_id=0)\n",
    "\n",
    "# Creating a data collator's object\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for calculating BLEU score metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "# Function for preprocessing data for calculating metrics\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "# Main function for calculating metrics: BLEU score and average lenght of generated sentences\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)  # Replacing -100 tokens(if any) on pad tokens of the tokenizer\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True) # Decoding tokens to words\n",
    "    \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)  # Replacing -100 tokens(if any) on pad tokens of the tokenizer\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True) # Decoding tokens to words\n",
    "    \n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels) # Prepare sentences for calculating metrics\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)   # Calculating BLEU score for each sentence\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)    # Calculating average lenght of generated sentences \n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing training hyperparameters and configuring training process with consequent launching it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e7b6df9887469e8ac44a201a828e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63832 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8458, 'learning_rate': 1.9844592054142124e-05, 'epoch': 0.03}\n",
      "{'loss': 1.701, 'learning_rate': 1.9688244140869785e-05, 'epoch': 0.06}\n",
      "{'loss': 1.6346, 'learning_rate': 1.9531582905125955e-05, 'epoch': 0.09}\n",
      "{'loss': 1.5996, 'learning_rate': 1.937492166938213e-05, 'epoch': 0.13}\n",
      "{'loss': 1.5754, 'learning_rate': 1.9218260433638303e-05, 'epoch': 0.16}\n",
      "{'loss': 1.5606, 'learning_rate': 1.9061599197894474e-05, 'epoch': 0.19}\n",
      "{'loss': 1.5458, 'learning_rate': 1.8904937962150648e-05, 'epoch': 0.22}\n",
      "{'loss': 1.5276, 'learning_rate': 1.8748276726406818e-05, 'epoch': 0.25}\n",
      "{'loss': 1.5308, 'learning_rate': 1.8591615490662992e-05, 'epoch': 0.28}\n",
      "{'loss': 1.4942, 'learning_rate': 1.8434954254919163e-05, 'epoch': 0.31}\n",
      "{'loss': 1.5057, 'learning_rate': 1.8278293019175337e-05, 'epoch': 0.34}\n",
      "{'loss': 1.4961, 'learning_rate': 1.8121631783431507e-05, 'epoch': 0.38}\n",
      "{'loss': 1.4803, 'learning_rate': 1.796528387015917e-05, 'epoch': 0.41}\n",
      "{'loss': 1.4853, 'learning_rate': 1.7808622634415342e-05, 'epoch': 0.44}\n",
      "{'loss': 1.4745, 'learning_rate': 1.7651961398671513e-05, 'epoch': 0.47}\n",
      "{'loss': 1.4616, 'learning_rate': 1.7495300162927687e-05, 'epoch': 0.5}\n",
      "{'loss': 1.4656, 'learning_rate': 1.7338952249655348e-05, 'epoch': 0.53}\n",
      "{'loss': 1.4645, 'learning_rate': 1.718229101391152e-05, 'epoch': 0.56}\n",
      "{'loss': 1.4614, 'learning_rate': 1.7025629778167693e-05, 'epoch': 0.6}\n",
      "{'loss': 1.4545, 'learning_rate': 1.6868968542423863e-05, 'epoch': 0.63}\n",
      "{'loss': 1.451, 'learning_rate': 1.6712620629151524e-05, 'epoch': 0.66}\n",
      "{'loss': 1.4415, 'learning_rate': 1.6556586038350672e-05, 'epoch': 0.69}\n",
      "{'loss': 1.4408, 'learning_rate': 1.6399924802606843e-05, 'epoch': 0.72}\n",
      "{'loss': 1.4366, 'learning_rate': 1.6243263566863017e-05, 'epoch': 0.75}\n",
      "{'loss': 1.4345, 'learning_rate': 1.608660233111919e-05, 'epoch': 0.78}\n",
      "{'loss': 1.43, 'learning_rate': 1.592994109537536e-05, 'epoch': 0.81}\n",
      "{'loss': 1.4241, 'learning_rate': 1.5773279859631535e-05, 'epoch': 0.85}\n",
      "{'loss': 1.4236, 'learning_rate': 1.5616618623887706e-05, 'epoch': 0.88}\n",
      "{'loss': 1.4098, 'learning_rate': 1.5459957388143876e-05, 'epoch': 0.91}\n",
      "{'loss': 1.415, 'learning_rate': 1.530329615240005e-05, 'epoch': 0.94}\n",
      "{'loss': 1.4001, 'learning_rate': 1.5146634916656224e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21625d5d14ba426e8e6ec46c0a5bbfe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.306068778038025, 'eval_bleu': 32.844, 'eval_gen_len': 15.2113, 'eval_runtime': 7134.6231, 'eval_samples_per_second': 8.946, 'eval_steps_per_second': 0.559, 'epoch': 1.0}\n",
      "{'loss': 1.3761, 'learning_rate': 1.4989973680912395e-05, 'epoch': 1.0}\n",
      "{'loss': 1.3226, 'learning_rate': 1.4833312445168569e-05, 'epoch': 1.03}\n",
      "{'loss': 1.3055, 'learning_rate': 1.4676651209424741e-05, 'epoch': 1.07}\n",
      "{'loss': 1.3019, 'learning_rate': 1.4519989973680915e-05, 'epoch': 1.1}\n",
      "{'loss': 1.3184, 'learning_rate': 1.4363642060408573e-05, 'epoch': 1.13}\n",
      "{'loss': 1.3254, 'learning_rate': 1.4206980824664747e-05, 'epoch': 1.16}\n",
      "{'loss': 1.3049, 'learning_rate': 1.4050319588920917e-05, 'epoch': 1.19}\n",
      "{'loss': 1.3084, 'learning_rate': 1.3893658353177091e-05, 'epoch': 1.22}\n",
      "{'loss': 1.3093, 'learning_rate': 1.3736997117433264e-05, 'epoch': 1.25}\n",
      "{'loss': 1.3146, 'learning_rate': 1.3580649204160923e-05, 'epoch': 1.28}\n",
      "{'loss': 1.3018, 'learning_rate': 1.3423987968417095e-05, 'epoch': 1.32}\n",
      "{'loss': 1.306, 'learning_rate': 1.3267640055144758e-05, 'epoch': 1.35}\n",
      "{'loss': 1.3157, 'learning_rate': 1.3110978819400928e-05, 'epoch': 1.38}\n",
      "{'loss': 1.3251, 'learning_rate': 1.29543175836571e-05, 'epoch': 1.41}\n",
      "{'loss': 1.3199, 'learning_rate': 1.2797656347913275e-05, 'epoch': 1.44}\n",
      "{'loss': 1.3114, 'learning_rate': 1.2641308434640932e-05, 'epoch': 1.47}\n",
      "{'loss': 1.2969, 'learning_rate': 1.2484647198897106e-05, 'epoch': 1.5}\n",
      "{'loss': 1.2985, 'learning_rate': 1.2327985963153279e-05, 'epoch': 1.54}\n",
      "{'loss': 1.2918, 'learning_rate': 1.2171324727409451e-05, 'epoch': 1.57}\n",
      "{'loss': 1.3138, 'learning_rate': 1.2014663491665623e-05, 'epoch': 1.6}\n",
      "{'loss': 1.3079, 'learning_rate': 1.1858002255921797e-05, 'epoch': 1.63}\n",
      "{'loss': 1.2988, 'learning_rate': 1.1701654342649455e-05, 'epoch': 1.66}\n",
      "{'loss': 1.2984, 'learning_rate': 1.1544993106905629e-05, 'epoch': 1.69}\n",
      "{'loss': 1.2948, 'learning_rate': 1.13883318711618e-05, 'epoch': 1.72}\n",
      "{'loss': 1.2907, 'learning_rate': 1.1231670635417973e-05, 'epoch': 1.75}\n",
      "{'loss': 1.3163, 'learning_rate': 1.1075009399674146e-05, 'epoch': 1.79}\n",
      "{'loss': 1.2888, 'learning_rate': 1.091834816393032e-05, 'epoch': 1.82}\n",
      "{'loss': 1.2982, 'learning_rate': 1.076168692818649e-05, 'epoch': 1.85}\n",
      "{'loss': 1.2734, 'learning_rate': 1.0605025692442663e-05, 'epoch': 1.88}\n",
      "{'loss': 1.2956, 'learning_rate': 1.0448364456698837e-05, 'epoch': 1.91}\n",
      "{'loss': 1.2836, 'learning_rate': 1.0291703220955007e-05, 'epoch': 1.94}\n",
      "{'loss': 1.2679, 'learning_rate': 1.0135041985211181e-05, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b064568341b4d959371b614768fffe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2573332786560059, 'eval_bleu': 33.7363, 'eval_gen_len': 15.1838, 'eval_runtime': 6916.3259, 'eval_samples_per_second': 9.229, 'eval_steps_per_second': 0.577, 'epoch': 2.0}\n",
      "{'loss': 1.2763, 'learning_rate': 9.978380749467352e-06, 'epoch': 2.01}\n",
      "{'loss': 1.2245, 'learning_rate': 9.821719513723526e-06, 'epoch': 2.04}\n",
      "{'loss': 1.2209, 'learning_rate': 9.665058277979698e-06, 'epoch': 2.07}\n",
      "{'loss': 1.2109, 'learning_rate': 9.508710364707357e-06, 'epoch': 2.1}\n",
      "{'loss': 1.214, 'learning_rate': 9.35204912896353e-06, 'epoch': 2.13}\n",
      "{'loss': 1.214, 'learning_rate': 9.195387893219704e-06, 'epoch': 2.16}\n",
      "{'loss': 1.2291, 'learning_rate': 9.038726657475874e-06, 'epoch': 2.19}\n",
      "{'loss': 1.2159, 'learning_rate': 8.882065421732048e-06, 'epoch': 2.22}\n",
      "{'loss': 1.2248, 'learning_rate': 8.725717508459707e-06, 'epoch': 2.26}\n",
      "{'loss': 1.2237, 'learning_rate': 8.56905627271588e-06, 'epoch': 2.29}\n",
      "{'loss': 1.2102, 'learning_rate': 8.412395036972052e-06, 'epoch': 2.32}\n",
      "{'loss': 1.2158, 'learning_rate': 8.255733801228226e-06, 'epoch': 2.35}\n",
      "{'loss': 1.2162, 'learning_rate': 8.099072565484397e-06, 'epoch': 2.38}\n",
      "{'loss': 1.2275, 'learning_rate': 7.942411329740569e-06, 'epoch': 2.41}\n",
      "{'loss': 1.2239, 'learning_rate': 7.785750093996743e-06, 'epoch': 2.44}\n",
      "{'loss': 1.2054, 'learning_rate': 7.629088858252914e-06, 'epoch': 2.48}\n",
      "{'loss': 1.2057, 'learning_rate': 7.472427622509087e-06, 'epoch': 2.51}\n",
      "{'loss': 1.2044, 'learning_rate': 7.31576638676526e-06, 'epoch': 2.54}\n",
      "{'loss': 1.2334, 'learning_rate': 7.159105151021431e-06, 'epoch': 2.57}\n",
      "{'loss': 1.2119, 'learning_rate': 7.002443915277604e-06, 'epoch': 2.6}\n",
      "{'loss': 1.2047, 'learning_rate': 6.8457826795337766e-06, 'epoch': 2.63}\n",
      "{'loss': 1.2193, 'learning_rate': 6.689121443789949e-06, 'epoch': 2.66}\n",
      "{'loss': 1.2066, 'learning_rate': 6.532460208046122e-06, 'epoch': 2.69}\n",
      "{'loss': 1.1985, 'learning_rate': 6.375798972302294e-06, 'epoch': 2.73}\n",
      "{'loss': 1.2076, 'learning_rate': 6.219137736558466e-06, 'epoch': 2.76}\n",
      "{'loss': 1.2075, 'learning_rate': 6.063103145757614e-06, 'epoch': 2.79}\n",
      "{'loss': 1.2133, 'learning_rate': 5.906441910013787e-06, 'epoch': 2.82}\n",
      "{'loss': 1.2141, 'learning_rate': 5.749780674269959e-06, 'epoch': 2.85}\n",
      "{'loss': 1.2022, 'learning_rate': 5.593432760997619e-06, 'epoch': 2.88}\n",
      "{'loss': 1.1996, 'learning_rate': 5.4367715252537915e-06, 'epoch': 2.91}\n",
      "{'loss': 1.204, 'learning_rate': 5.280110289509965e-06, 'epoch': 2.95}\n",
      "{'loss': 1.185, 'learning_rate': 5.123449053766136e-06, 'epoch': 2.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911d125031154d7095f9bc2c9871c08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2376044988632202, 'eval_bleu': 34.1361, 'eval_gen_len': 15.1473, 'eval_runtime': 6921.0957, 'eval_samples_per_second': 9.222, 'eval_steps_per_second': 0.576, 'epoch': 3.0}\n",
      "{'loss': 1.1857, 'learning_rate': 4.967101140493797e-06, 'epoch': 3.01}\n",
      "{'loss': 1.1594, 'learning_rate': 4.8104399047499685e-06, 'epoch': 3.04}\n",
      "{'loss': 1.1528, 'learning_rate': 4.653778669006142e-06, 'epoch': 3.07}\n",
      "{'loss': 1.1554, 'learning_rate': 4.497117433262314e-06, 'epoch': 3.1}\n",
      "{'loss': 1.1547, 'learning_rate': 4.340456197518486e-06, 'epoch': 3.13}\n",
      "{'loss': 1.1543, 'learning_rate': 4.1837949617746585e-06, 'epoch': 3.16}\n",
      "{'loss': 1.1514, 'learning_rate': 4.027133726030832e-06, 'epoch': 3.2}\n",
      "{'loss': 1.147, 'learning_rate': 3.870472490287004e-06, 'epoch': 3.23}\n",
      "{'loss': 1.1811, 'learning_rate': 3.713811254543176e-06, 'epoch': 3.26}\n",
      "{'loss': 1.1619, 'learning_rate': 3.5571500187993486e-06, 'epoch': 3.29}\n",
      "{'loss': 1.1492, 'learning_rate': 3.4004887830555213e-06, 'epoch': 3.32}\n",
      "{'loss': 1.1531, 'learning_rate': 3.2438275473116936e-06, 'epoch': 3.35}\n",
      "{'loss': 1.1502, 'learning_rate': 3.0874796340393537e-06, 'epoch': 3.38}\n",
      "{'loss': 1.1624, 'learning_rate': 2.9308183982955256e-06, 'epoch': 3.42}\n",
      "{'loss': 1.1469, 'learning_rate': 2.7741571625516983e-06, 'epoch': 3.45}\n",
      "{'loss': 1.1607, 'learning_rate': 2.6178092492793584e-06, 'epoch': 3.48}\n",
      "{'loss': 1.1643, 'learning_rate': 2.461148013535531e-06, 'epoch': 3.51}\n",
      "{'loss': 1.1551, 'learning_rate': 2.3044867777917034e-06, 'epoch': 3.54}\n",
      "{'loss': 1.1573, 'learning_rate': 2.1478255420478757e-06, 'epoch': 3.57}\n",
      "{'loss': 1.1467, 'learning_rate': 1.9914776287755362e-06, 'epoch': 3.6}\n",
      "{'loss': 1.1512, 'learning_rate': 1.8348163930317083e-06, 'epoch': 3.63}\n",
      "{'loss': 1.1665, 'learning_rate': 1.6781551572878806e-06, 'epoch': 3.67}\n",
      "{'loss': 1.157, 'learning_rate': 1.5214939215440533e-06, 'epoch': 3.7}\n",
      "{'loss': 1.1557, 'learning_rate': 1.3648326858002256e-06, 'epoch': 3.73}\n",
      "{'loss': 1.1654, 'learning_rate': 1.2081714500563981e-06, 'epoch': 3.76}\n",
      "{'loss': 1.1567, 'learning_rate': 1.0515102143125706e-06, 'epoch': 3.79}\n",
      "{'loss': 1.1671, 'learning_rate': 8.948489785687431e-07, 'epoch': 3.82}\n",
      "{'loss': 1.1656, 'learning_rate': 7.38501065296403e-07, 'epoch': 3.85}\n",
      "{'loss': 1.1469, 'learning_rate': 5.818398295525755e-07, 'epoch': 3.89}\n",
      "{'loss': 1.1498, 'learning_rate': 4.25178593808748e-07, 'epoch': 3.92}\n",
      "{'loss': 1.1625, 'learning_rate': 2.6851735806492044e-07, 'epoch': 3.95}\n",
      "{'loss': 1.1443, 'learning_rate': 1.1185612232109289e-07, 'epoch': 3.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcda7d07bb794fdc9b936ebbc8d0a608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2308039665222168, 'eval_bleu': 34.3492, 'eval_gen_len': 15.1585, 'eval_runtime': 6708.1919, 'eval_samples_per_second': 9.515, 'eval_steps_per_second': 0.595, 'epoch': 4.0}\n",
      "{'train_runtime': 39750.9662, 'train_samples_per_second': 25.691, 'train_steps_per_second': 1.606, 'train_loss': 1.2919149440636775, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63832, training_loss=1.2919149440636775, metrics={'train_runtime': 39750.9662, 'train_samples_per_second': 25.691, 'train_steps_per_second': 1.606, 'train_loss': 1.2919149440636775, 'epoch': 4.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = '../models/Bart_2e-5_16_10_MaxTokenLength' # Path for models checkpoints\n",
    "\n",
    "# All training arguments/hyperparameters\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    generation_max_length = max_len + 5,\n",
    "    weight_decay=1e-6,\n",
    ")\n",
    "\n",
    "# Trainer object creation\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Starting training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model checkpoint and generating detoxified sentence\n",
    "    I hope that there is no need to comment anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = BartForConditionalGeneration.from_pretrained(\"../models/Bart-base-4096_detox/final_checkpoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"Do you want to know how stupid and ugly you are?\".lower().split(), \n",
    "                      max_length=max_len, truncation=True, is_split_into_words=True, return_tensors=\"pt\").input_ids\n",
    "outputs = test_model.generate(input_ids, max_new_tokens=max_len + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " do you want to know how bad you are?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
