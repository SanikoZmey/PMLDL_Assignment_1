## `Main references that helped to complete the code for preprocessing data, creating & configuring + fine-tuning & validation of model phases`
#### [T5-small short reference (Hugging face)](https://huggingface.co/t5-small)
#### [T5 reference (Hugging face)](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Config)
#### [Bart-base-4096 short reference (Hugging face)](https://huggingface.co/ccdv/lsg-bart-base-4096)
#### [Seq2SeqTrainer reference (Hugging face)](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainer)
#### [How to visualize logs of Seq2SeqTrainer (Hugging face discussion)](https://discuss.huggingface.co/t/how-to-read-the-logs-created-by-hugging-face-trainer/32279)
#### [T5-Small generation reference (Habr - RUS)](https://habr.com/ru/articles/762140/)
#### [Partial help with coding training pipeline (StackOverflow)](https://stackoverflow.com/questions/62446827/how-do-i-train-a-encoder-decoder-model-for-a-translation-task-using-hugging-face)
#### [DataCollator reference (Hugging face)](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForSeq2Seq)
#### [Data preprocessing (Hugging face)](https://huggingface.co/docs/datasets/use_dataset#tokenize-text)
#### [Model evaluation info (Google Cloud)](https://cloud.google.com/translate/automl/docs/evaluate)  

## `From the following references I got some ideas(that most probably will not be implemented)`
    I red about several quite interesting and complex (for me) stratagies to build and train models for seq2seq text generation. More info about it can be found in reports. 
#### [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
#### [Text Detoxification using Large Pre-trained Neural Models](https://arxiv.org/abs/2109.08914)